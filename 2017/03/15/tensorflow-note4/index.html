<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>tensorflow note4 | HIT_CS_ITNLP_ZGD</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="How to structure your TensorFlow modelPhase 1: assemble your graph

Define placeholders for input and output
Define the weights
Define the inference model
Define loss function
Define optimizer">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow note4">
<meta property="og:url" content="http://yoursite.com/2017/03/15/tensorflow-note4/index.html">
<meta property="og:site_name" content="HIT_CS_ITNLP_ZGD">
<meta property="og:description" content="How to structure your TensorFlow modelPhase 1: assemble your graph

Define placeholders for input and output
Define the weights
Define the inference model
Define loss function
Define optimizer">
<meta property="og:image" content="http://yoursite.com/img/tensorflow_note3_1.png">
<meta property="og:updated_time" content="2017-08-06T07:54:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tensorflow note4">
<meta name="twitter:description" content="How to structure your TensorFlow modelPhase 1: assemble your graph

Define placeholders for input and output
Define the weights
Define the inference model
Define loss function
Define optimizer">
<meta name="twitter:image" content="http://yoursite.com/img/tensorflow_note3_1.png">
  
    <link rel="alternate" href="/atom.xml" title="HIT_CS_ITNLP_ZGD" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">HIT_CS_ITNLP_ZGD</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-tensorflow-note4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/15/tensorflow-note4/" class="article-date">
  <time datetime="2017-03-15T13:07:52.000Z" itemprop="datePublished">2017-03-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      tensorflow note4
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="How-to-structure-your-TensorFlow-model"><a href="#How-to-structure-your-TensorFlow-model" class="headerlink" title="How to structure your TensorFlow model"></a>How to structure your TensorFlow model</h3><p>Phase 1: assemble your graph</p>
<ol>
<li>Define placeholders for input and output</li>
<li>Define the weights</li>
<li>Define the inference model</li>
<li>Define loss function</li>
<li>Define optimizer<a id="more"></a>
Phase 2: execute the computation</li>
<li>Initialize all model variables for the first time.</li>
<li>Feed in the training data. Might involve randomizing the order of data samples.</li>
<li>Execute the inference model on the training data, so it calculates for each training input example the output with the current model parameters.</li>
<li>Compute the cost</li>
<li>Adjust the model parameters to minimize/maximize the cost depending on the model.<br><img src="/img/tensorflow_note3_1.png" alt=""></li>
</ol>
<p>对于如果要建立一个skim-gram模型，根据以上的做法：<br>Phase 1: Assemble the graph</p>
<ol>
<li><p>Define placeholders for input and output<br>Input is the center word and output is the target (context) word. Instead of using one-hot vectors, we input the index of those words directly. For example, if the center word is the 1001th word in the vocabulary, we input the number 1001. Each sample input is a scalar, the placeholder for BATCH_SIZE sample inputs with have shape[BATCH_SIZE].<br>Similar, the placeholder for BATCH_SIZE sample outputs with have shape [BATCH_SIZE].</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])</div><div class="line">target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])</div></pre></td></tr></table></figure>
</li>
</ol>
<p>Note that our center_words and target_words being fed in are both scalars – we feed in their corresponding indices in our vocabulary.</p>
<ol>
<li><p>Define the weight (in this case, embedding matrix)<br>Each row corresponds to the representation vector of one word. If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will have shape [VOCAB_SIZE,EMBED_SIZE]. We initialize the embedding matrix to value from a random distribution. In this case, let’s choose uniform distribution</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0))</div></pre></td></tr></table></figure>
</li>
<li><p>Inference (compute the forward path of the graph)<br>Our goal is to get the vector representations of words in our dictionary. Remember that the embed_matrix has dimension VOCAB_SIZE x EMBED_SIZE, with each row of the embedding matrix corresponds to the vector representation of the word at that index. So to get the representation of all the center words in the batch, we get the slice of all corresponding rows in the embedding matrix. TensorFlow provides a convenient method to do so called tf.nn.embedding_lookup().</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.embedding_lookup(params, ids, partition_strategy=&apos;mod&apos;, name=None,validate_indices=True, max_norm=None)</div></pre></td></tr></table></figure>
</li>
<li><p>Define the loss function</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true=1,sampled_values=None, remove_accidental_hits=False, partition_strategy=&apos;mod&apos;,name=&apos;nce_loss&apos;)</div><div class="line">nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],stddev=1.0 / EMBED_SIZE ** 0.5))</div><div class="line">nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]))</div><div class="line"></div><div class="line"></div><div class="line">loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,</div><div class="line">          biases=nce_bias,</div><div class="line">          labels=target_words,</div><div class="line">          inputs=embed,</div><div class="line">          num_sampled=NUM_SAMPLED,</div><div class="line">          num_classes=VOCAB_SIZE))</div></pre></td></tr></table></figure>
</li>
<li><p>Define optimizer<br>We will use the good old gradient descent.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)</div></pre></td></tr></table></figure>
</li>
</ol>
<p>Phase 2: Execute the computation<br>We will create a session then within the session, use the good old feed_dict to feed inputs and outputs into the placeholders, run the optimizer to minimize the loss, and fe3<br>  with tf.Session() as sess:<br>    sess.run(tf.global_variables_initializer())<br>    average_loss = 0.0<br>    for index in xrange(NUM_TRAIN_STEPS):<br>      batch = batch_gen.next()<br>      loss<em>batch, </em> = sess.run([loss, optimizer],<br>                      feed_dict={center_words: batch[0], target_words: batch[1]})<br>      average_loss += loss_batch<br>      if (index + 1) % 2000 == 0:<br>        print(‘Average loss at step {}: {:5.1f}’.format(index + 1,<br>            average_loss / (index + 1)))<br>```</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/03/15/tensorflow-note4/" data-id="cj6distzr001mekre0j03t3d5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow-note/">tensorflow_note</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/16/python与设计模式/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          python与设计模式
        
      </div>
    </a>
  
  
    <a href="/2017/03/15/Linux常用的一些指令/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Linux常用的一些指令</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/dl/">dl</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂/">杂</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/">deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deeplearning/">deeplearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/learn/">learn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/note/">note</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/read/">read</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow-note/">tensorflow_note</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/杂/">杂</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/自然语言处理/">自然语言处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/读书笔记/">读书笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/deep-learning/" style="font-size: 16.67px;">deep learning</a> <a href="/tags/deeplearning/" style="font-size: 10px;">deeplearning</a> <a href="/tags/learn/" style="font-size: 10px;">learn</a> <a href="/tags/note/" style="font-size: 11.67px;">note</a> <a href="/tags/python/" style="font-size: 13.33px;">python</a> <a href="/tags/read/" style="font-size: 10px;">read</a> <a href="/tags/tensorflow-note/" style="font-size: 15px;">tensorflow_note</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/杂/" style="font-size: 10px;">杂</a> <a href="/tags/自然语言处理/" style="font-size: 18.33px;">自然语言处理</a> <a href="/tags/读书笔记/" style="font-size: 11.67px;">读书笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/08/15/决策树/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/08/15/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2017/08/15/20170815杂/">20170815杂</a>
          </li>
        
          <li>
            <a href="/2017/08/06/推荐系统技术的简要了解/">推荐系统技术的简要了解</a>
          </li>
        
          <li>
            <a href="/2017/05/20/end2end-relation-extractio-lstm/">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Zheng GuiDong<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>